import csv
import os
import numpy as np
import scipy.sparse as sp
from sklearn.cluster import KMeans
from .utils import PlotCallback
from utils.utils import load_data_trunc
import tensorflow as tf
from .models import AE, VAE
from tensorflow.keras.optimizers import Adam
from tensorflow.keras.losses import MeanSquaredError
from tensorflow.keras.callbacks import EarlyStopping, CSVLogger
from utils.utils import save_results, salt_and_pepper, largest_eigval_smoothing_filter, preprocess_adj
from .utils import AlphaRateScheduler, alpha_scheduler, get_alpha, assign_clusters
import datetime


def run_kspace_grid_search():
    from tensorboard.plugins.hparams import api as hp
    seed = 123
    np.random.seed(seed)
    tf.random.set_seed(seed)

    adj, feature, gnd, idx_train, idx_val, idx_test = load_data_trunc("cora")
    gnd = np.argmax(gnd, axis=1)
    feature = feature.todense()
    feature = feature.astype(np.float32)
    adj = sp.coo_matrix(adj)
    adj_normalized = preprocess_adj(adj)

    h = largest_eigval_smoothing_filter(adj_normalized)

    HP_POWERS = hp.HParam('power', hp.Discrete([5, 6, 7, 8]))
    HP_BATCH = hp.HParam('batch', hp.Discrete([32, 64, 100, 200]))
    HP_DROPOUT = hp.HParam('dropout', hp.RealInterval(0.1, 0.2))
    HP_OPTIMIZER = hp.HParam('optimizer', hp.Discrete(['adam', 'sgd']))

    session_num = 0
    for power in HP_POWERS.domain.values:
        for batch in HP_BATCH.domain.values:
            for dropout_rate in (HP_DROPOUT.domain.min_value, HP_DROPOUT.domain.max_value):
                for optimizer in HP_OPTIMIZER.domain.values:
                    hparams = {
                        HP_POWERS: power,
                        HP_BATCH: batch,
                        HP_DROPOUT: dropout_rate,
                        HP_OPTIMIZER: optimizer,
                    }
                    run_name = "run-%d" % session_num
                    print('--- Starting trial: %s' % run_name)
                    print({h.name: hparams[h] for h in hparams})
                    logdir = 'logs/hparam_tuning/' + run_name

                    h_k = h ** hparams[HP_POWERS]
                    X = h_k.dot(feature)

                    model = AE(layers=2, dims=[200, 100], output_dim=X.shape[1], dropout=hparams[HP_DROPOUT])
                    model.compile(optimizer=hparams[HP_OPTIMIZER], loss=MeanSquaredError())
                    es = EarlyStopping(monitor='loss', patience=20)
                    model.fit(feature, X, epochs=500, batch_size=hparams[HP_BATCH], shuffle=True, callbacks=[es, tf.keras.callbacks.TensorBoard(logdir), hp.KerasCallback(logdir, hparams)], verbose=1)
                    mse = model.evaluate(feature, X)
                    tf.summary.scalar('mse', mse, step=1)
                    session_num += 1


def run_kspace(args):
    # import tensorflow as tf
    # with tf.device('/cpu:0'):
    seed = 123
    np.random.seed(seed)
    tf.random.set_seed(seed)

    adj, feature, gnd, idx_train, idx_val, idx_test = load_data_trunc(args.input)

    # convert one hot labels to integer ones
    if args.input != "wiki":
        gnd = np.argmax(gnd, axis=1)
        feature = feature.todense()
    feature = feature.astype(np.float32)

    adj = sp.coo_matrix(adj)
    adj_normalized = preprocess_adj(adj)

    h = largest_eigval_smoothing_filter(adj_normalized)
    h_k = h ** args.power
    X = h_k.dot(feature)

    for _ in range(args.repeats):
        kspace(args, feature, X, gnd)


def kspace(args, feature, X, gnd):
    save_location = 'output/{}_{}_{}_power{}_epochs{}_dims{}-batch{}-lr{}-drop{}'.format(args.input, args.method,
        args.model, args.power, args.epochs, ",".join([str(x) for x in args.dims]), args.batch_size, args.learning_rate, args.dropout)

    m = len(np.unique(gnd))
    # alphas = get_alpha(args.a_max, args.epochs, args.alpha)
    alphas = [0.0, 2.531512052442886e-07, 5.127109637602412e-07, 7.788415088463153e-07, 1.0517091807564772e-06, 1.3314845306682633e-06, 1.6183424272828307e-06, 1.912462166123581e-06, 2.2140275816016986e-06, 2.5232271619186445e-06, 2.840254166877414e-06, 3.1653067486762155e-06, 3.498588075760032e-06, 3.840306459807515e-06, 4.190675485932573e-06, 4.549914146182013e-06, 4.918246976412704e-06, 5.295904196633787e-06, 5.68312185490169e-06, 6.08014197485783e-06, 6.487212707001282e-06, 6.904588483790915e-06, 7.332530178673953e-06, 7.771305269140386e-06, 8.221188003905092e-06, 8.682459574322224e-06, 9.155408290138963e-06, 9.640329759698474e-06, 1.0137527074704767e-05, 1.0647310999664869e-05, 1.1170000166126749e-05, 1.1705921271834426e-05, 1.225540928492468e-05, 1.2818807653293042e-05, 1.3396468519259913e-05, 1.3988752939670982e-05, 1.45960311115695e-05, 1.521868260358148e-05, 1.5857096593158464e-05, 1.651167210982607e-05, 1.7182818284590453e-05, 1.787095460565851e-05, 1.857651118063164e-05, 1.9299929005337016e-05, 2.0041660239464334e-05, 2.0802168489180312e-05, 2.1581929096897683e-05, 2.2381429438379613e-05, 2.3201169227365482e-05, 2.4041660827908195e-05, 2.4903429574618416e-05, 2.57870141010158e-05, 2.6692966676192447e-05, 2.7621853549999115e-05, 2.8574255306969746e-05, 2.9550767229205775e-05, 3.0551999668446756e-05, 3.1578578427560076e-05, 3.263114515168819e-05, 3.371035772929759e-05, 3.4816890703380647e-05, 3.595143569306689e-05, 3.7114701825907425e-05, 3.8307416181102795e-05, 3.953032424395115e-05, 4.078419037180082e-05, 4.2069798271798496e-05, 4.338795149073177e-05, 4.473947391727201e-05, 4.612521029693158e-05, 4.7546026760057313e-05, 4.900281136319017e-05, 5.0496474644129466e-05, 5.202795019104867e-05, 5.3598195226018325e-05, 5.520819120330113e-05, 5.685894442279271e-05, 5.855148665899179e-05, 6.028687580589295e-05, 6.206619653820468e-05, 6.389056098930651e-05, 6.576110944636845e-05, 6.767901106306775e-05, 6.964546459034773e-05, 7.16616991256765e-05, 7.372897488127266e-05, 7.584858397177893e-05, 7.80218512218761e-05, 8.025013499434123e-05, 8.253482803906893e-05, 8.487735836358527e-05, 8.727919012559885e-05, 8.974182454814724e-05, 9.226680085791002e-05, 9.485569724727577e-05, 9.751013186076355e-05, 0.00010023176380641606, 0.00010302229419279585, 0.00010588346719223393, 0.00010881707113099401, 0.00011182493960703475, 0.00011490895263606814, 0.00011807103782663037, 0.00012131317158490017, 0.00012463738035001692, 0.00012804574186067096, 0.00013154038645375808, 0.0001351234983959092, 0.0001387973172487284, 0.00014256413926859159, 0.00014642631884188173, 0.00015038626995655678, 0.00015444646771097057, 0.00015860944986089036, 0.00016287781840567642, 0.00016725424121461643, 0.00017174145369443068, 0.00017634226049899007, 0.00018105953728231653, 0.00018589623249595997, 0.00019085536923187668, 0.00019594004711196038, 0.00020115344422540616, 0.0002064988191151206, 0.0002119795128144164, 0.00021759895093526732, 0.00022336064580942723, 0.00022926819868375142, 0.00023532530197109354, 0.00024153574155818367, 0.00024790339917193065, 0.00025443225480562806, 0.00026112638920657895, 0.00026798998642669667, 0.00027502733643767287, 0.00028224283781234943, 0.0002896410004739703, 0.00029722644851502925, 0.00030500392308747937, 0.00031297828536610683, 0.00032115451958692316, 0.00032953773616247557, 0.0003381331748760203, 0.0003469462081565588, 0.0003559823444367799, 0.00036524723159600997, 0.0003747466604903214, 0.00038448656857200536, 0.000394473043600674, 0.0004047123274483076, 0.00041521082000062784, 0.0004259750831572351, 0.0004370118449330084, 0.0004483280036633348, 0.0004599306323157929, 0.00047182698291098823, 0.00048402449105530194, 0.0004965307805883865, 0.0005093536683483145, 0.0005225011690573563, 0.0005359815003314424, 0.0005498030878164416, 0.000563974570454462, 0.0005785048058834742, 0.0005934028759736201, 0.0006086780925036788, 0.0006243400029812335, 0.0006403983966101697, 0.0006568633104092516, 0.0006737450354855816, 0.0006910541234668786, 0.0007088013930965908, 0.0007269979369959579, 0.0007456551285972663, 0.000764784629252609, 0.0007843983955226134, 0.0008045086866496815, 0.000825128072220412, 0.000846269440022006, 0.0008679460040975486, 0.0008901713130052181, 0.0009129592582865789, 0.0009363240831492407, 0.0009602803913693336, 0.0009848431564193386, 0.001010027730826997, 0.0010358498557711422, 0.0010623256709204449, 0.0010894717245212354, 0.0011173049837406924, 0.0011458428452718768, 0.0011751031462072334, 0.0012051041751873497, 0.0012358646838319537, 0.0012674038984602893, 0.0012997415321081862, 0.0013328977968493554, 0.0013668934164285872, 0.0014017496392147687, 0.0014374882514818102, 0.0014741315910257661, 0.0015117025611266963, 0.0015502246448639513, 0.0015897219197938655, 0.0016302190729990185, 0.0016717414165184545, 0.0017143149031685434, 0.0017579661427643246, 0.0018027224187515123, 0.0018486117052595417, 0.0018956626845863, 0.0019439047651255096, 0.0019933680997479184, 0.0020440836046478317, 0.0020960829786667457, 0.0021493987231061412, 0.002204064162041872, 0.002260113463152781, 0.002317581659076621, 0.002376504669306597, 0.0024369193226422043, 0.002498863380208446, 0.0025623755590577497, 0.0026274955563693903, 0.002694264074261528, 0.00276272284523134, 0.0028329146582392094, 0.002904883385453202, 0.002978674009670603, 0.0030543326524336434, 0.003131906602856942, 0.0032114443471847657, 0.0032929955990964894, 0.003376611330779273, 0.003462343804787348, 0.003550246606707791, 0.0036403746786532894, 0.003732784353602722, 0.003827533390611119, 0.0039246810109109615, 0.004024287934927351, 0.004126416420230288, 0.004231130300447646, 0.004338495025163282, 0.004448577700825172, 0.00456144713268909, 0.004677173867824171, 0.004795830239207094, 0.004917490410932563, 0.005042230424568291, 0.005170128246683421, 0.0053012638175802115, 0.005435719101259295, 0.005573578136649855, 0.005714927090136715, 0.005859854309417089, 0.006008450378720823, 0.006160808175428437, 0.006317022928122536, 0.006477192276108785, 0.0066414163304436185, 0.0068097977365069555, 0.0069824417381588585, 0.007159456243520394, 0.007340951892419733, 0.007527042125545614, 0.0077178432553515, 0.007913474538754621, 0.008114058251675434, 0.00831971976546408, 0.008530587625261515, 0.00874679363034446, 0.008968472916504185, 0.009195764040510803, 0.00942880906671579, 0.009667753655846767, 0.009912747156050263, 0.010163942696239114, 0.010421497281803032, 0.010685571892742133, 0.010956331584284586, 0.011233945590051607, 0.011518587427833887, 0.011810435008045909, 0.012109670744925775, 0.01241648167054996, 0.012731059551734541, 0.01305360100989563, 0.01338430764394418, 0.013723386156291912, 0.014071048482046957, 0.014427511921480306, 0.014792999275845466, 0.015167738986636517, 0.015551965278371549, 0.01594591830499049, 0.01634984429995927, 0.016763995730173637, 0.01718863145375923, 0.017624016881866385, 0.018070424144560632, 0.018528132260912985, 0.018997427313395807, 0.019478602626693733, 0.01997195895104119, 0.02047780465020098, 0.02099645589420178, 0.0215282368569546, 0.022073479918872093, 0.02263252587461715, 0.023205724146110572, 0.02379343300093126, 0.02439601977624501, 0.02501386110840223, 0.025647343168348012, 0.02629686190299136, 0.0269628232826851, 0.027645643554970404, 0.028345749504745107, 0.029063578721018255, 0.029799579870417287, 0.030554212977619497, 0.03132794971288226, 0.03212127368685238, 0.032934680752838404, 0.03376867931673535, 0.03462379065479455, 0.035500549239438524, 0.03639950307332359, 0.037321214031859284, 0.038266258214399065, 0.039235226304320754, 0.04022872393822313, 0.041247372084468485, 0.04229180743130795, 0.043362682784832705, 0.04446066747699858, 0.04558644778398061, 0.04674072735511793, 0.04792422765271738, 0.04913768840299135, 0.05038186805841017, 0.051657544271759955, 0.05296551438220084, 0.05430659591362978, 0.055681627085660046, 0.05709146733753509, 0.058536997865306556, 0.06001912217261029, 0.061538766635385496, 0.06309688108089025, 0.0646944393813738, 0.0663324400627789, 0.06801190692885288, 0.06973388970105816, 0.07149946467468295, 0.07330973539155995, 0.07516583332981655, 0.07706891861108527, 0.07902018072561742, 0.08102083927575385, 0.0830721447382152, 0.08517537924569119, 0.08733185738821507, 0.08954292703482508, 0.0918099701760271, 0.09413440378758273, 0.09651768071616591, 0.09896129058743929, 0.10146676073711842, 0.10403565716560724, 0.10666958551679802, 0.10937019208165193, 0.11213916482718417, 0.1149782344514982, 0.1178891754655292, 0.12087380730216989, 0.1239339954534762, 0.12707165263666026, 0.13028873998960072, 0.13358726829661874, 0.13696929924528214, 0.14043694671502832, 0.14399237809840734, 0.14763781565577294, 0.15137553790426728, 0.15520788104196936, 0.15913724040809332, 0.1631660719801545, 0.16729689390903504, 0.17153228809290985, 0.17587490179101825, 0.18032744927828526, 0.1848927135418332, 0.1895735480204387, 0.19437287838802467, 0.19929370438230298, 0.2043391016797062, 0.20951222381778678, 0.21481630416628048, 0.2202546579480672, 0.22583068431129466, 0.23154786845395467, 0.23740978380224645, 0.24342009424408423, 0.24958255641914598, 0.255901022066897, 0.26237944043405037, 0.26902186074297596, 0.27583243472259583, 0.2828154192033498, 0.28997517877785484, 0.2973161885289144, 0.30484303682659303, 0.3125604281960965, 0.32047318625825255, 0.3285862567444333, 0.33690471058779486, 0.3454337470927781, 0.3541786971848422, 0.3631450267424664, 0.37233834001350424, 0.38176438311801725, 0.3914290476397895, 0.4013383743087585, 0.4114985567766677, 0.421915945488303, 0.4325970516507232, 0.4435485513029792, 0.45477728948885526, 0.4662902845352433, 0.4780947324388286, 0.4901980113638175, 0.5026076862535334, 0.5153315135587513, 0.5283774460857319, 0.5417536379669876, 0.5554684497578751, 0.5695304536622188, 0.583948438890216, 0.5987314171519782, 0.6138886282901481, 0.6294295460550949, 0.6453638840263217, 0.661701601683767, 0.6784529106328034, 0.6956282809868279, 0.7132384479114162, 0.7312944183341559, 0.7498074778243325, 0.7687891976467777, 0.7882514419942903, 0.8082063754031361, 0.8286664703562839, 0.8496445150791118, 0.8711536215324633, 0.8932072336080559, 0.9158191355313411, 0.9390034604771008, 0.9627746994031401, 0.987147710107605, 1.0121377265155986, 1.037760368200868, 1.0640316501485543, 1.0909679927650775, 1.1185862321414233, 1.1469036305762557, 1.1759378873654045, 1.2057071498645076, 1.2362300248316989, 1.2675255900574343, 1.299613406288742, 1.3325135294553103, 1.3662465232051095, 1.4008334717573343, 1.4362959930807286, 1.472656252405527, 1.5099369760774328, 1.5481614657623302, 1.5873536130105859, 1.6275379141900392, 1.6687394857970435, 1.7109840801550749, 1.7542981015107855, 1.7987086225375135, 1.8442434012565827, 1.8909308983869817, 1.9388002951342207, 1.9878815114295476, 2.038205224630876, 2.08980288869713, 2.142706753848003, 2.196949886721379, 2.2525661910410673, 2.3095904288077316, 2.3680582420262684, 2.4280061749832362, 2.489471697088201, 2.552493226293349, 2.6171101531059597]

    # CREATE MODEL
    if args.model == 'ae' or args.model == 'dae':
        model = AE(dims=args.dims, output_dim=X.shape[1], dropout=args.dropout, num_centers=m, alphas=alphas)
        model.compile(optimizer=Adam(lr=args.learning_rate))
    else:  # args.model == 'vae' or args.model == 'dvae'
        model = VAE(dims=args.dims, output_dim=X.shape[1], dropout=args.dropout)
        model.compile(optimizer=Adam(lr=args.learning_rate))

    # TRAINING OR LOAD MODEL IF IT EXISTS
    if not os.path.exists(save_location):
        model, acc, nmi, f1, ari = train(args, feature, X, gnd, model)

        if args.save:
            os.makedirs(save_location)
            model.save_weights(save_location + '/checkpoint')
            write_results(args, acc, nmi, f1, ari)
    else:
        print('Model already exists. Loading it...')
        model.load_weights(save_location + '/checkpoint')

    # SAVE EMBEDDINGS
    if args.save:
        embeds, _ = model.embed(feature)
        embeds_to_save = [e.numpy() for e in embeds]
        save_results(args, save_location, embeds_to_save)

    # tsne(embeds, gnd, args)

def train(args, feature, X, gnd, model):
    """
    Model training
    :param args: cli arguments
    :param feature: original feature matrix
    :param X: the smoothed matrix
    :param gnd: ground truth labels
    :param model: the nn to be trained
    """

    m = len(np.unique(gnd))  # number of clusters according to ground truth
    # Cluster = KMeans(n_clusters=m)

    # ADD NOISE IN CASE OF DENOISING MODELS
    if args.model == 'ae' or args.model == 'vae':
        input = feature
    else:
        input = salt_and_pepper(feature)

    # CALLBACKS
    # es = EarlyStopping(monitor='loss', patience=args.early_stopping)
    # csv_location = 'output/kspace/logs/{}amax_{}step.csv'.format(args.a_max, args.alpha)
    # csv_logger = CSVLogger(csv_location)
    pc = PlotCallback(gnd, input)
    alpha_cb = AlphaRateScheduler(alpha_scheduler)

    logdir = "logs/fit/" + datetime.datetime.now().strftime("%Y%m%d-%H%M%S")
    tensorboard_callback = tf.keras.callbacks.TensorBoard(log_dir=logdir)
    # TRAINING
    # input is the plain feature matrix and output is the k-order convoluted. The model reconstructs the convolution!
    print('Training model for {}-order convolution'.format(args.power))
    # model.fit(input, X, epochs=args.epochs, batch_size=args.batch_size, shuffle=True, callbacks=[alpha_cb, tensorboard_callback, pc], verbose=1)
    model.fit(input, X, epochs=args.epochs, batch_size=args.batch_size, shuffle=True,
              callbacks=[alpha_cb, tensorboard_callback], verbose=1)
    embeds = model.embed(input)
    centers = model.centers
    # db, acc, nmi, f1, ari, centers = clustering(Cluster, embeds, gnd)
    acc, nmi, f1, ari = assign_clusters(embeds, centers, gnd)
    print("Optimization Finished!")
    print("ACC: {} F1: {} NMI: {} ARI: {}".format(acc, f1, nmi, ari))

    return model, acc, nmi, f1, ari


def write_results(args, ac, nm, f1, ari):
    file_exists = os.path.isfile('output/kspace/results.csv')
    with open('output/kspace/results.csv', 'a') as f:
        columns = ['Dataset', 'Model', 'Dimensions', 'Epochs', 'Batch Size', 'Learning Rate', 'Dropout',
                   'A Max', 'A Type', 'Power', 'Accuracy', 'NMI', 'F1', 'ARI']
        writer = csv.DictWriter(f, delimiter=',', lineterminator='\n', fieldnames=columns)

        if not file_exists:
            writer.writeheader()  # file doesn't exist yet, write a header
        writer.writerow({'Dataset': args.input, 'Model': args.model, 'Dimensions': ",".join([str(x) for x in args.dims]),
                         'Epochs': args.epochs, 'Batch Size': args.batch_size, 'Learning Rate': args.learning_rate,
                         'Dropout': args.dropout, 'A Max': args.a_max, 'A Type': args.alpha, 'Power': args.power,
                         'Accuracy': ac, 'NMI': nm, 'F1': f1, 'ARI': ari})